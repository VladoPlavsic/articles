{
  "id": 1,
  "title": "How a Single Line of Code Made All the Difference",
  "subtitle": "A deep dive into how a costly JOIN brought an ETL pipeline to a crawl. And how one tiny fix turned hours into minutes.",
  "content": "**Abstract:**\n\nIn this article, I will tell you a real life story and a bit of how PostgreSQL handles JOINs and some of the performance pitfalls you might encounter when writing unoptimized queries.\n\nTo illustrate this, I’ll describe a real-world scenario where a minor SQL tweak drastically improved performance in an ETL pipeline — something my entire team had accepted as “just slow.”\n\n --- \n\n**The Challenge:**\n\nA Massive Dataset\nImagine a system managing tens of thousands of objects, each with millions of time-series records tracking various statuses over time. Our goal was to aggregate these records by different parameters — per object, per time period, etc.\n\nWhile PostgreSQL handled the workload decently, things got complicated as our dataset approached 100 million records. To tackle this, we decided to offload data to Amazon Redshift, an analytics database, using an ETL pipeline:\n\n 1.Extract data from PostgreSQL.\n\n 2.Transform it into a denormalized format.\n\n 3.Load it into Redshift.\n\nThis pipeline involved processing vast amounts of data, converting time-series statuses into consecutive `start_ts` and `end_ts` timestamps from multiple tables.\n\n**The Initial ETL Implementation:**\n\nThe ETL process followed these steps:\n\n 1.Select all objects for which data should be loaded.\n\n 2.Process each object separately (to reduce the strain on Postgres).\n\n 3.Build transformation queries.\n\n 4.Stream query results into a CSV file and upload them to S3.\n\n 5.Load the data into Redshift.\n\n Everything worked fine — until our latest production release.\n\n --- \n\n**The Problem: A Performance Nightmare**\n\nFollowing the release, two major issues surfaced:\n\n **1.Full Data Resync** — We needed to run full resync for 3 years worth of data due to an old bug. With thousands of objects, each taking 7–10 minutes (sometimes 20+ minutes), this was a painfully slow process.\n\n **2.Data Explosion** — A new bug caused certain objects to generate 500,000+ status updates per day, further slowing things down.\n\nI was responsible for handling this re-sync. After a week of manually running scripts and tweaking parameters, I started questioning my life choices. Maybe I should open a beach bar and mix cocktails instead?\n\nThe team had already optimized the transformation process as much as possible — or so we thought. Everyone accepted that “this is just slow, and there’s nothing we can do right now.”\n\nStill, I decided to take one last look.\n\n --- \n\n**The Investigation: Finding the Bottleneck**\n\nOne particular object was failing even when processed for a single day’s worth of data. I could have chunked it into smaller periods and moved on, but I was too bored with the repetitive work. So, I added logging to pinpoint the slowest part of the pipeline.\n\nSurprisingly, the timeout was happening right at the start — on the very first attempt to load a single record from the transformed query. That didn’t make sense. Running the query manually returned results within minutes, yet in the pipeline, it timed out after 20+ minutes.\n\nSomething was wrong.\n\nI dove into the transformation query module and found this:\n\n ```sql\nSELECT q.*, o.name\nFROM query_built_for_single_object AS q\nINNER JOIN object AS o ON q.object_id = o.id\nLEFT JOIN (\n    SELECT object_id, array_agg(tag) FROM tags GROUP BY object_id\n) AS t ON q.object_id = t.object_id;\n``` \n\n At first glance, this looked fine. But then it hit me.\n\n --- \n\n**The Root Cause: A Costly JOIN**\n\nPostgreSQL is usually great at optimizing queries, especially when using indexed joins. However, when a join is performed on a column without an index, things go south quickly.\n\nLet’s break it down:\n\n **+Indexed JOINs**: PostgreSQL scans the outer table and looks up matching rows in the indexed column of the inner table.\n\n **+Non-Indexed JOINs**: It performs a nested loop join, checking every row from the outer table against every row in the inner table.\n\nIn our case:\n\n +The outer query (`q`) contained 1.5 million rows.\n\n +The inner subquery (`tags`) contained only 200 rows.\n\n +Because the join condition didn’t filter tags properly, PostgreSQL compared every row in `q` to every row in `tags`, leading to 300 million operations.\n\nNo wonder the pipeline was crawling!\n\n --- \n\n**The Fix: One Simple WHERE Clause**\n\nThe solution was almost laughably simple:\n\n```sql\nLEFT JOIN (\n    SELECT object_id, array_agg(tag)\n    FROM tags\n    WHERE object_id = ?  -- Filter before aggregating!\n    GROUP BY object_id\n) AS t ON q.object_id = t.object_id;\n```\n\nSince our pipeline processes one object at a time, there was no need to aggregate tags for all objects in the database. This tiny `WHERE` clause reduced the number of comparisons from 300 million to just 1.5 million.\n\nAnd just like that, the problem disappeared.\n\n --- \n\n **The Lesson: Always Question Assumptions**\n\nThis experience taught me a valuable lesson: even when everyone is convinced that a process cannot be optimized further, it’s worth double-checking.\n\nWorst case? You confirm there’s nothing left to improve. Best case? You save countless hours of computation with a single line of code.\n\nNext time someone tells you, “That’s just the way it is,” have a closer look. You might just find your own one-line miracle."
}
